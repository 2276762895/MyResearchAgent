import streamlit as st
import os
# --- ã€æ–°å¢ã€‘è®¾ç½®å›½å†…é•œåƒ ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# --- é¡µé¢è®¾ç½® ---
st.set_page_config(page_title="ç§‘ç ”è®ºæ–‡æ™ºèƒ½åŠ©æ‰‹", layout="wide")
st.title("ğŸ“ è¯¾é¢˜ç»„ç§‘ç ”åŠ©æ‰‹ - æ–‡çŒ®é˜…è¯»ç‰ˆ")

# --- ä¾§è¾¹æ ï¼šè®¾ç½®ä¸æ–‡ä»¶ä¸Šä¼  ---
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    # è¾“å…¥ DeepSeek API Key
    api_key = st.text_input("è¯·è¾“å…¥ DeepSeek API Key", type="password")
    st.markdown("[ç‚¹å‡»ç”³è¯· DeepSeek API](https://platform.deepseek.com/)")

    st.divider()

    st.header("ğŸ“‚ ä¸Šä¼ æ–‡çŒ®")
    uploaded_file = st.file_uploader("ä¸Šä¼ PDFæ–‡ä»¶", type=["pdf"])


# --- æ ¸å¿ƒé€»è¾‘å‡½æ•° ---

@st.cache_resource
def process_pdf(file, api_key):
    if not file or not api_key:
        return None

    print("1. [å¼€å§‹] æ­£åœ¨ä¿å­˜ä¸´æ—¶æ–‡ä»¶...")
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(file.getvalue())
        tmp_path = tmp_file.name

    print(f"2. [åŠ è½½] æ­£åœ¨è¯»å–PDF: {tmp_path} ...")
    loader = PyPDFLoader(tmp_path)
    docs = loader.load()
    print(f"   -> PDFè¯»å–å®Œæˆï¼Œå…± {len(docs)} é¡µ")

    print("3. [åˆ‡åˆ†] æ­£åœ¨åˆ‡åˆ†æ–‡æœ¬...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    print(f"   -> åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(splits)} ä¸ªæ–‡æœ¬å—")

    print("4. [æ¨¡å‹] æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹ (è¿™ä¸€æ­¥æœ€å®¹æ˜“å¡)...")
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        print("   -> æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"   -> âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise e

    print("5. [å­˜å‚¨] æ­£åœ¨å†™å…¥å‘é‡æ•°æ®åº“ (FAISS)...")
    try:
        # ä½¿ç”¨ FAISS æ›¿ä»£ Chromaï¼Œä¸éœ€è¦ SQLite æ”¯æŒï¼Œä¸ä¼šé—ªé€€
        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
        print("   -> æ•°æ®åº“å†™å…¥æˆåŠŸï¼(å·²åˆ‡æ¢ä¸º FAISS)")
    except Exception as e:
        print(f"   -> âŒ FAISS å†™å…¥å¤±è´¥: {e}")
        raise e

    print("6. [è¿æ¥] æ­£åœ¨åˆå§‹åŒ– DeepSeek...")
    llm = ChatOpenAI(
        model_name="deepseek-chat",
        openai_api_key=api_key,
        openai_api_base="https://api.deepseek.com",
        temperature=0.3
    )

    print("7. [å®Œæˆ] å‡†å¤‡å°±ç»ªï¼")

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )

    os.remove(tmp_path)
    return qa_chain

# --- ä¸»ç•Œé¢é€»è¾‘ ---

if uploaded_file and api_key:
    with st.spinner("æ­£åœ¨é˜…è¯»è®ºæ–‡ï¼Œè¯·ç¨å€™... (ç¬¬ä¸€æ¬¡åŠ è½½æ¨¡å‹å¯èƒ½éœ€è¦1åˆ†é’Ÿ)"):
        # å¤„ç†PDF
        qa_chain = process_pdf(uploaded_file, api_key)

    st.success("âœ… è®ºæ–‡å·²è¯»å–ï¼Œå¿«æ¥é—®æˆ‘é—®é¢˜å§ï¼")

    # åˆå§‹åŒ–èŠå¤©å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # è·å–ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ"):
        # 1. æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. è°ƒç”¨æ¨¡å‹å›ç­”
        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                response = qa_chain.invoke({"query": prompt})
                answer = response["result"]
                source_docs = response["source_documents"]

                # æ‹¼æ¥å¼•ç”¨æ¥æºï¼ˆå¯é€‰ï¼‰
                source_text = "\n\n> **å‚è€ƒç‰‡æ®µï¼š**\n"
                for i, doc in enumerate(source_docs):
                    source_text += f"> {i + 1}. Page {doc.metadata['page']}: {doc.page_content[:100]}...\n"

                full_response = answer + source_text
                st.markdown(full_response)

        # 3. ä¿å­˜åŠ©æ‰‹å›ç­”
        st.session_state.messages.append({"role": "assistant", "content": full_response})

else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥API Keyå¹¶ä¸Šä¼ PDFæ–‡ä»¶å¼€å§‹ã€‚")